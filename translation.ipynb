{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaOSrvct7dX8"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8GwJkoNj7dYA"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate sacrebleu torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qhd_GW5J7dYB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omSuPoLY7dYB"
   },
   "source": [
    "## Q1: Dataset Preparation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VtQHtK5j7dYB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRQ7Hsrj7dYC"
   },
   "source": [
    "We use the ```load_dataset()``` function to download the dataset. Replace the dummy arguments to download the wmt14 dataset for fr-en translation as provided here: https://huggingface.co/datasets/wmt/wmt14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "klc-jtLi7dYC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01b0d02236840459443255c20636014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wmt/wmt14\",  \"fr-en\", split='train[:15000]')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OajGgO-R7dYC"
   },
   "source": [
    "Now, we split the dataset into training and testing splits. This is done using the ```train_test_split``` function. Replace the dummy arguments with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9oCpCOAv7dYC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = dataset.train_test_split(train_size=0.8, seed=42)\n",
    "split_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU3rB0eR7dYD"
   },
   "source": [
    "Define the test dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F0KvRQL07dYD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = split_datasets[\"test\"]\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykFRxwh57dYD"
   },
   "source": [
    "Now, follow the same process to split the train dataset to training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kynutdwu7dYD"
   },
   "outputs": [],
   "source": [
    "split_to_val = split_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "train_dataset = split_to_val[\"train\"]\n",
    "eval_dataset = split_to_val[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J7AypmH7dYD"
   },
   "source": [
    "## Q2 Prepare for training RNNs (10)\n",
    "In this part, you are required to define the tokenizers for english and french, tokenize the data, and define the dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaR1qKZs7dYD"
   },
   "source": [
    "Choose and initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qfFHDk3D7dYD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gus/anaconda3/envs/pt/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\") # CHOOSE AN APPROPRIATE MULTILINGUAL MODEL such as https://huggingface.co/google-bert/bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITKX-cza7dYD"
   },
   "source": [
    "You will need to create a pytorch dataset to process the tokens in the required format. Complete the implementation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uCKYlKu07dYD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, input_size, output_size):\n",
    "        source_texts = [text[\"translation\"][\"fr\"] for text in dataset]\n",
    "        target_texts = [text[\"translation\"][\"en\"] for text in dataset]\n",
    "        self.source_sentences = tokenizer(source_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        self.target_sentences = tokenizer(target_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_sentences[idx], self.target_sentences[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUl1vQj07dYE"
   },
   "source": [
    "Initialize the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-JfiZAH7dYE"
   },
   "source": [
    "Get the vocab size from the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aR1CEzJN7dYE"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size # This size is used somewhere in the model, think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u-Hqb62L7dYE"
   },
   "outputs": [],
   "source": [
    "train_dataset_rnn = TranslationDataset(train_dataset, vocab_size, vocab_size)\n",
    "eval_dataset_rnn = TranslationDataset(eval_dataset, vocab_size, vocab_size)\n",
    "test_dataset_rnn = TranslationDataset(test_dataset, vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8uvrc0R7dYE"
   },
   "source": [
    "Initialize and define the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8W3qVX2N7dYE"
   },
   "outputs": [],
   "source": [
    "#Instantiate the DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(train_dataset_rnn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset_rnn, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset_rnn, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X03nj-qd7dYE"
   },
   "source": [
    "## Q3: Implementing RNNs (10)\n",
    "Define the RNN model as an encoder-decoder RNN for the task of translation in the cell below. You may refer: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6iYwjZXt7dYE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kZcyOabn7dYE"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding layer for input tokens\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # Encoder RNN (using GRU for better performance than basic RNN)\n",
    "        self.encoder_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Decoder RNN\n",
    "        self.decoder_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Final linear layer to predict output tokens\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = self._init_hidden(batch_size, x.device)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(x))  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Encoder\n",
    "        _, hidden = self.encoder_rnn(embedded, hidden)\n",
    "        \n",
    "        # For decoder input, we use the first token (<SOS>) from target sequence\n",
    "        # For this implementation, we'll use zeros as initial input\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # We'll collect outputs for each time step\n",
    "        outputs = []\n",
    "        \n",
    "        # Get sequence length from input\n",
    "        max_length = x.size(1)\n",
    "        \n",
    "        # Decoder loop - one step at a time\n",
    "        for t in range(max_length):\n",
    "            # Pass through decoder\n",
    "            output, hidden = self.decoder_rnn(decoder_input, hidden)\n",
    "            \n",
    "            # Apply linear layer to get vocabulary distribution\n",
    "            output = self.out(output)\n",
    "            \n",
    "            # Save output\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Use output as next input (teacher forcing would use actual target here)\n",
    "            decoder_input = self.embedding(output.argmax(2))\n",
    "        \n",
    "        # Stack outputs into a single tensor [batch_size, seq_len, output_size]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SnP9NUtK7dYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqRNN(\n",
       "  (embedding): Embedding(119547, 256)\n",
       "  (encoder_rnn): GRU(256, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(256, 256, batch_first=True)\n",
       "  (out): Linear(in_features=256, out_features=119547, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2SeqRNN(input_size=vocab_size, hidden_size=256, output_size=vocab_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MZb5OZc7dYE"
   },
   "source": [
    "## Q4: Training RNNs (15)\n",
    "In this question, you will define the hyperparameters, loss and optimizer for training. You will then implement a custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "INYhxibp7dYE"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7cPIUCd7dYE"
   },
   "source": [
    "define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FeCoP4DC7dYE"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam  # Replace IMPORT_OPTIMIZER with Adam\n",
    "from torch.nn import CrossEntropyLoss  # Replace IMPORT_LOSS_FUNCTION with CrossEntropyLoss\n",
    "\n",
    "num_train_epochs = 5  # Set NUM_EPOCHS to a reasonable value like 5\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "criterion = CrossEntropyLoss(ignore_index=0)  # Use 0 as pad token ID to ignore in loss calculation\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Standard learning rate for Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2w-eirJ7dYE"
   },
   "source": [
    "Write the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "waBwBdVx7dYE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                                                                                                                                                                        | 0/6750 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.82 GiB. GPU 0 has a total capacity of 5.79 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 4.10 GiB memory in use. Of the allocated memory 3.96 GiB is allocated by PyTorch, and 41.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.82 GiB. GPU 0 has a total capacity of 5.79 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 4.10 GiB memory in use. Of the allocated memory 3.96 GiB is allocated by PyTorch, and 41.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_src, batch_tgt in train_dataloader:\n",
    "        # Move tensors to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            batch_src = batch_src.cuda()\n",
    "            batch_tgt = batch_tgt.cuda()\n",
    "            \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_src)\n",
    "        \n",
    "        # Reshape outputs and targets for loss calculation\n",
    "        # outputs: [batch_size, seq_len, vocab_size]\n",
    "        # targets: [batch_size, seq_len]\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        targets = batch_tgt.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch}: Average Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_src, batch_tgt in eval_dataloader:\n",
    "            # Move tensors to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                batch_src = batch_src.cuda()\n",
    "                batch_tgt = batch_tgt.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_src)\n",
    "            \n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            targets = batch_tgt.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_eval_loss += loss.item()\n",
    "            total_batches += 1\n",
    "    \n",
    "    avg_loss = total_eval_loss / total_batches\n",
    "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-e1x_Lm7dYF"
   },
   "source": [
    "## Q5: Evaluating RNNs for Machine Translation (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrE3V_P17dYF"
   },
   "source": [
    "Implement the calculation of BLEU-1,2,3,4 scores using the ```sacrebleu``` library for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xddoMFY17dYF"
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "\n",
    "model.eval()\n",
    "bleu1, bleu2, bleu3, bleu4 = None, None, None, None\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    # Complete the testing loop\n",
    "\n",
    "print(\"BLEU-1: \", bleu1)\n",
    "print(\"BLEU-2: \", bleu2)\n",
    "print(\"BLEU-3: \", bleu3)\n",
    "print(\"BLEU-4: \", bleu4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpaHAQWp7dYF"
   },
   "source": [
    "Congratulations! You can now work with RNNs for the task of Machine Translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeBy8PO37dYF"
   },
   "source": [
    "## Q6: Prepare for training transformers (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vauBDs_a7dYJ"
   },
   "source": [
    "In this part we cover the initial setup required before training transformer this including data preprocessing and setting up data collators and loaders.\n",
    "\n",
    "Ensure you have loaded the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW5Y83Ew7dYK"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMGCBjca7dYK"
   },
   "source": [
    "We will begin by tokenizing the data. Based on your model selection load the appropriate tokenizer. We are using models from AutoModelForSeq2SeqLM in this assignment. You can checkout all the available models here: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr49H_Bq7dYK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"\" #Select a model of your choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(REPLACE_WITH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_FwXurd7dYK"
   },
   "source": [
    "We will need to tokenize both our input and outputs. Thus we make use of pre_process() function to generate tokenized model inputs and targets. Ensure you use truncation and padding! The max length will be 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF8_JKSS7dYK"
   },
   "outputs": [],
   "source": [
    "##Implement the preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
    "    targets = [example[SET_RIGHT_LANG] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer() #Instantitate tokenizer to generate model outputs\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IwNWQEf8Fru"
   },
   "outputs": [],
   "source": [
    "tokenized_train_data = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H42Bthkh8GHS"
   },
   "outputs": [],
   "source": [
    "tokenized_val_data = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9KiU8Uw8jpI"
   },
   "source": [
    "We remove the column 'translation' as we do not require it for training. Also often having columns other than we created using the preprocess_function may lead to errors during training. Since model might get confused which inputs it needs to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z_WfysF8Jqz"
   },
   "outputs": [],
   "source": [
    "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES0vpD308KFK"
   },
   "outputs": [],
   "source": [
    "tokenized_train_data.set_format(\"torch\")\n",
    "tokenized_val_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBKQon-E7dYK"
   },
   "source": [
    "To construct batches of training data for model training, we require collators that set the properties for the batches and data loaders that generate the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J_Bs4bT7dYK"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq() #INSTANTIATE THE COLLATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ErHyp6A7dYK"
   },
   "outputs": [],
   "source": [
    "#Instantiate the DataLoader for training and evaluation data\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(, batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq75QwZr7dYK"
   },
   "source": [
    "## Q7) Choosing & Loading the Model (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Cr5Z0T7dYK"
   },
   "source": [
    "Choose a pre-trained transformer model that you will use for fine-tuning on the translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZY0-m7L7dYK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(REPLACE_WITH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsaL0EAC7dYK"
   },
   "source": [
    "## Q8) Training the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKlkWxxh7dYK"
   },
   "source": [
    "Now, that we have are data tokenized and ready in batches and model fixed. We will begin with training this model. To do so we must setup the right hyperparameters, then proceed to implment the training loop to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e0JWoMi7dYL"
   },
   "source": [
    "For training we require an optimizer and a scheduler to manage the learning rate during the training. Let's set them up before our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxwICUp17dYL"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = NUM_EPOCHS\n",
    "num_training_steps = NUM_STEPS\n",
    "\n",
    "optimizer = SETUP_Adam_OPTIMIZER\n",
    "lr_scheduler = SETUP_SCHEDULER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqdgWb_Q7dYL"
   },
   "source": [
    "Finally, we are here!\n",
    "\n",
    "In the loop during training you will run a forward pass, compute the loss, compute the gradients, and then update the weights. (Don't foregt to set gradient to zero!)\n",
    "\n",
    "During the eval phase we simply do a forward pass and compute the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0iDbp0x7dYL"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        ## Complete the training loop\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      ### Complete the evaluation phase\n",
    "\n",
    "    avg_loss = None\n",
    "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_7JA8gD7dYL"
   },
   "source": [
    "Congratulations!! On completing the training. Now don't forget to save your model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqTmBLMG7dYL"
   },
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = SET_OUTPUT_DIR\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCrwRPf-7dYL"
   },
   "source": [
    "## Q9) Evaluating Transformer for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsteZEw77dYL"
   },
   "source": [
    "We will now test our trained model and analyze its performance using BLEU-1, 2, 3, 4 scores from the sacrebleu library. You will create a task evaluator for translation, load and process the test dataset, and compute the results on an existing trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNP8tJ3Y9Yna"
   },
   "source": [
    "Below we load a model trained for french to english translation. You can read more about it here: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-fr-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUIYns7E9S2j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HRtKCKh9mqc"
   },
   "source": [
    "Initialize an evaluator for translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR_Kc8NN7dYL"
   },
   "outputs": [],
   "source": [
    "## Load Evaluator for translation\n",
    "from evaluate import evaluator\n",
    "task_evaluator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL8zPaFz7dYL"
   },
   "source": [
    "We will need to change our test dataset by having specific input and target columns. Thus we will use split_translation to split the translation column into two columns 'en' and 'fr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMCG74Vr7dYL"
   },
   "outputs": [],
   "source": [
    "#  Implement the split function\n",
    "def split_translations(example):\n",
    "    en_text = example[][]\n",
    "    fr_text = example[][]\n",
    "    example['en'] =\n",
    "    example['fr'] =\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpUlKUm17dYL"
   },
   "outputs": [],
   "source": [
    "test_data = test_dataset.map(split_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA9Vt7kF7dYL"
   },
   "source": [
    "You can now go ahead and compute the results by appropriately setting up the task_evaluator.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV6gQhFP7dYL"
   },
   "outputs": [],
   "source": [
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline= MODEL,\n",
    "    data= DATA,\n",
    "    metric=METRIC,\n",
    "    input_column=COLUMN,\n",
    "    label_column=COLUMN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEv4BBNq7dYM"
   },
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvPJif197dYM"
   },
   "source": [
    "## Q10) Inferencing on Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2VpQtGW7dYM"
   },
   "source": [
    "Let's check out how well this trained model's translation skills are. You can use try with a few french sentence and see how well it translates.\n",
    "\n",
    "To do so we will setup a pipline using the existing trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6mVfvWC7dYM"
   },
   "source": [
    "Loading the tokenizer and model for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nojJlgVf7dYM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53K25pUn7dYM"
   },
   "source": [
    "Setup the pipeline for translation using your model and tokenizer. You can read about pipelines here: https://huggingface.co/docs/transformers/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKp9pNoQ7dYM"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Instatiate a pipeline for Translation using the model and tokenizer\n",
    "pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdV-e6sM7dYM"
   },
   "source": [
    "Translate the given sentence using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc8Y01YC7dYM"
   },
   "outputs": [],
   "source": [
    "input_text = \"REPLACE WITH A SENTENCE IN FRENCH.\"\n",
    "translation_result = pipeline(REPLACE_WITH_TEXT_TO_TRANSLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eRCXrNN7dYM"
   },
   "outputs": [],
   "source": [
    "print(translation_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
